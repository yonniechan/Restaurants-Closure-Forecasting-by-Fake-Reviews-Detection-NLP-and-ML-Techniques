{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = open(\"data/yelp_academic_dataset_review.json\", encoding = \"utf8\")\n",
    "reviews_data_0 = []\n",
    "reviews_data_1 = []\n",
    "reviews_data_2 = []\n",
    "reviews_data_3 = []\n",
    "reviews_data_4 = []\n",
    "reviews_data_5 = []\n",
    "reviews_data_6 = []\n",
    "c = 0\n",
    "for line in data_file:\n",
    "    if c < 1000000:\n",
    "        reviews_data_0.append(json.loads(line))\n",
    "    elif c >= 1000000 and c < 2000000:\n",
    "        reviews_data_1.append(json.loads(line))\n",
    "    elif c >= 2000000 and c < 3000000:\n",
    "        reviews_data_2.append(json.loads(line))\n",
    "    elif c >= 3000000 and c < 4000000:\n",
    "        reviews_data_3.append(json.loads(line))\n",
    "    elif c >= 4000000 and c < 5000000:\n",
    "        reviews_data_4.append(json.loads(line))\n",
    "    elif c >= 5000000 and c < 6000000:\n",
    "        reviews_data_5.append(json.loads(line))\n",
    "    else:\n",
    "        reviews_data_6.append(json.loads(line))\n",
    "    c+=1\n",
    "\n",
    "reviews_df_0 = pd.DataFrame(reviews_data_0)\n",
    "reviews_df_1 = pd.DataFrame(reviews_data_1)\n",
    "reviews_df_2 = pd.DataFrame(reviews_data_2)\n",
    "reviews_df_3 = pd.DataFrame(reviews_data_3)\n",
    "reviews_df_4 = pd.DataFrame(reviews_data_4)\n",
    "reviews_df_5 = pd.DataFrame(reviews_data_5)\n",
    "reviews_df_6 = pd.DataFrame(reviews_data_6)\n",
    "\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = open(\"data/yelp_academic_dataset_business.json\", encoding = \"utf8\")\n",
    "business_data = []\n",
    "for line in data_file:\n",
    "    business_data.append(json.loads(line))\n",
    "\n",
    "business_df = pd.DataFrame(business_data)\n",
    "\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_df.sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out reviews for restaurants not in PA \n",
    "business_list = business_df[business_df['state']=='PA']['business_id']\n",
    "# df = reviews_df_1[reviews_df_1['business_id'].isin(business_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_time_to_feature(df):\n",
    "\n",
    "    df['date'] =  pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    df['year'] = pd.DatetimeIndex(df['date']).year\n",
    "    df['month'] = pd.DatetimeIndex(df['date']).month\n",
    "    df['day'] = pd.DatetimeIndex(df['date']).day\n",
    "    df['hour'] = pd.DatetimeIndex(df['date']).hour\n",
    "    df['min'] = pd.DatetimeIndex(df['date']).minute\n",
    "    df['sec'] = pd.DatetimeIndex(df['date']).second\n",
    "\n",
    "# split_time_to_feature(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract reivews left in 2000-2018\n",
    "# df = df.loc[(df['year'] >= 2000) & (df['year'] <= 2018)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    # Creating a new column in the dataset for the length of the reviews\n",
    "    df['length_of_reviews'] = df['text'].apply(len)\n",
    "\n",
    "    # Creating a new column in the dataset for the number of words in the reviews\n",
    "    df['num_of_words'] = df['text'].apply(lambda str:len(nltk.word_tokenize(str)))\n",
    "\n",
    "    # Creating a new column in the dataset for the number of sentences in the reviews\n",
    "    df['num_of_sentences'] = df['text'].apply(lambda paragraph:len(nltk.sent_tokenize(paragraph)))\n",
    "\n",
    "    df['capital_words'] = df['text'].apply(lambda sen:len(re.findall(r'\\b[A-Z]+\\b', sen)))\n",
    "    df['capital_words_ratio'] = df['capital_words']/df['num_of_words']\n",
    "    df.drop(columns='capital_words', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with texts in reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the reviews - stemed, remove stopwords and punctuation\n",
    "def clean_text(text):\n",
    "\n",
    "    ## Remove puncuation\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    ## Remove stop words\n",
    "    nostop = [word for word in nopunc.split() if word.lower() not in stopwords.words('english') and len(word) >= 3]\n",
    "    text = ' '.join(nostop)\n",
    "   \n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)    \n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    text = ps.stem(text)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = lemmatizer.lemmatize(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# df['cleaned_text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis using textblob \n",
    "# df['sentiment_polarity'] = df['cleaned_text'].apply(lambda w:TextBlob(w).polarity)\n",
    "# df['sentiment_subjectivity'] = df['cleaned_text'].apply(lambda w:TextBlob(w).subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_list = [reviews_df_0, reviews_df_1, reviews_df_2, reviews_df_3,\n",
    "               reviews_df_4, reviews_df_5, reviews_df_6]\n",
    "for i, reviews in enumerate(reviews_list):\n",
    "    df = reviews[reviews['business_id'].isin(business_list)]\n",
    "\n",
    "    split_time_to_feature(df)\n",
    "\n",
    "    # Extract reivews left in 2000-2018\n",
    "    df = df.loc[(df['year'] >= 2000) & (df['year'] <= 2018)]\n",
    "\n",
    "    feature_engineering(df)\n",
    "\n",
    "    df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "    # Sentiment analysis using textblob \n",
    "    df['sentiment_polarity'] = df['cleaned_text'].apply(lambda w:TextBlob(w).polarity)\n",
    "    df['sentiment_subjectivity'] = df['cleaned_text'].apply(lambda w:TextBlob(w).subjectivity)\n",
    "\n",
    "    df.to_csv(f'data/cleaned_review_data_{i}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9cbb33fb0dd98d60d7542c1e0a8ec9389a1bc71302aaca21e62e688716f63114"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
